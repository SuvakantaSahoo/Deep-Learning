{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOhT0RP4cVaPmzyRXVnscPd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ohRHuWWWyjnk","executionInfo":{"status":"ok","timestamp":1668306646969,"user_tz":360,"elapsed":22831,"user":{"displayName":"Rohan Deb","userId":"07798907649003110334"}},"outputId":"15fa7686-d945-44cd-a792-357c6769fab7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puGOCWIgzBHp","executionInfo":{"status":"ok","timestamp":1668305591016,"user_tz":360,"elapsed":7,"user":{"displayName":"Rohan Deb","userId":"07798907649003110334"}},"outputId":"07e8446e-3197-45ef-f2c4-4d7ca3a8879b"},"source":["cd /content/drive/My Drive/Colab Notebooks/Machine Translation"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/Machine Translation\n"]}]},{"cell_type":"code","metadata":{"id":"gBvRP46XzDQW"},"source":["import string\n","import re\n","from pickle import dump\n","from unicodedata import normalize\n","from numpy import array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nreoWAG5JL4g"},"source":["# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, mode='rt', encoding='utf-8')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEaqKO9lJPUE"},"source":["# split a loaded document into sentences\n","def to_pairs(doc):\n","\tlines = doc.strip().split('\\n')\n","\tpairs = [line.split('\\t') for line in  lines]\n","\treturn pairs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xYX4LasrJSJs"},"source":["# clean a list of lines\n","def clean_pairs(lines):\n","\tcleaned = list()\n","\t# prepare regex for char filtering\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor pair in lines:\n","\t\tclean_pair = list()\n","\t\tfor line in pair:\n","\t\t\t# normalize unicode characters\n","\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n","\t\t\tline = line.decode('UTF-8')\n","\t\t\t# tokenize on white space\n","\t\t\tline = line.split()\n","\t\t\t# convert to lowercase\n","\t\t\tline = [word.lower() for word in line]\n","\t\t\t# remove punctuation from each token\n","\t\t\tline = [word.translate(table) for word in line]\n","\t\t\t# remove non-printable chars form each token\n","\t\t\tline = [re_print.sub('', w) for w in line]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tline = [word for word in line if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tclean_pair.append(' '.join(line))\n","\t\tcleaned.append(clean_pair)\n","\treturn array(cleaned)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"par33x-OJUcD"},"source":["# save a list of clean sentences to file\n","def save_clean_data(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EK_p6AzTJW1R","executionInfo":{"status":"ok","timestamp":1668305603731,"user_tz":360,"elapsed":9539,"user":{"displayName":"Rohan Deb","userId":"07798907649003110334"}},"outputId":"313c79f6-d907-4b6a-b477-6276505a4ec5"},"source":["# load dataset\n","filename = 'deu.txt'\n","doc = load_doc(filename)\n","# split into english-german pairs\n","pairs = to_pairs(doc)\n","# clean sentences\n","clean_pairs = clean_pairs(pairs)\n","# save clean pairs to file\n","save_clean_data(clean_pairs, 'english-german.pkl')\n","# spot check\n","for i in range(100):\n","\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: english-german.pkl\n","[hi] => [hallo]\n","[hi] => [gru gott]\n","[run] => [lauf]\n","[wow] => [potzdonner]\n","[wow] => [donnerwetter]\n","[fire] => [feuer]\n","[help] => [hilfe]\n","[help] => [zu hulf]\n","[stop] => [stopp]\n","[wait] => [warte]\n","[hello] => [hallo]\n","[i try] => [ich probiere es]\n","[i won] => [ich hab gewonnen]\n","[i won] => [ich habe gewonnen]\n","[smile] => [lacheln]\n","[cheers] => [zum wohl]\n","[freeze] => [keine bewegung]\n","[freeze] => [stehenbleiben]\n","[got it] => [verstanden]\n","[got it] => [einverstanden]\n","[he ran] => [er rannte]\n","[he ran] => [er lief]\n","[hop in] => [mach mit]\n","[hug me] => [druck mich]\n","[hug me] => [nimm mich in den arm]\n","[hug me] => [umarme mich]\n","[i fell] => [ich fiel]\n","[i fell] => [ich fiel hin]\n","[i fell] => [ich sturzte]\n","[i fell] => [ich bin hingefallen]\n","[i fell] => [ich bin gesturzt]\n","[i know] => [ich wei]\n","[i lied] => [ich habe gelogen]\n","[i lost] => [ich habe verloren]\n","[im] => [ich bin jahre alt]\n","[im] => [ich bin]\n","[im ok] => [mir gehts gut]\n","[im ok] => [es geht mir gut]\n","[no way] => [unmoglich]\n","[no way] => [das gibts doch nicht]\n","[no way] => [ausgeschlossen]\n","[no way] => [in keinster weise]\n","[really] => [wirklich]\n","[really] => [echt]\n","[really] => [im ernst]\n","[thanks] => [danke]\n","[try it] => [versuchs]\n","[why me] => [warum ich]\n","[ask tom] => [frag tom]\n","[ask tom] => [fragen sie tom]\n","[ask tom] => [fragt tom]\n","[be cool] => [entspann dich]\n","[be fair] => [sei nicht ungerecht]\n","[be fair] => [sei fair]\n","[be nice] => [sei nett]\n","[be nice] => [seien sie nett]\n","[beat it] => [geh weg]\n","[beat it] => [hau ab]\n","[beat it] => [verschwinde]\n","[beat it] => [verdufte]\n","[beat it] => [mach dich fort]\n","[beat it] => [zieh leine]\n","[beat it] => [mach dich vom acker]\n","[beat it] => [verzieh dich]\n","[beat it] => [verkrumele dich]\n","[beat it] => [troll dich]\n","[beat it] => [zisch ab]\n","[beat it] => [pack dich]\n","[beat it] => [mach ne fliege]\n","[beat it] => [schwirr ab]\n","[beat it] => [mach die sause]\n","[beat it] => [scher dich weg]\n","[beat it] => [scher dich fort]\n","[call me] => [ruf mich an]\n","[come in] => [komm herein]\n","[come in] => [herein]\n","[come on] => [komm]\n","[come on] => [kommt]\n","[come on] => [mach schon]\n","[come on] => [macht schon]\n","[get out] => [raus]\n","[go away] => [geh weg]\n","[go away] => [hau ab]\n","[go away] => [verschwinde]\n","[go away] => [verdufte]\n","[go away] => [mach dich fort]\n","[go away] => [zieh leine]\n","[go away] => [mach dich vom acker]\n","[go away] => [verzieh dich]\n","[go away] => [verkrumele dich]\n","[go away] => [troll dich]\n","[go away] => [zisch ab]\n","[go away] => [pack dich]\n","[go away] => [mach ne fliege]\n","[go away] => [schwirr ab]\n","[go away] => [mach die sause]\n","[go away] => [scher dich weg]\n","[go away] => [scher dich fort]\n","[go away] => [geh weg]\n","[go away] => [verpiss dich]\n"]}]},{"cell_type":"code","metadata":{"id":"3zBLfzQXJY5o"},"source":["from pickle import load\n","from pickle import dump\n","from numpy.random import rand\n","from numpy.random import shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"psa1f-ZRJshT"},"source":["def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n"," \n","# save a list of clean sentences to file\n","def save_clean_data(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0lM2sgRgJuhU","executionInfo":{"status":"ok","timestamp":1668305608015,"user_tz":360,"elapsed":2054,"user":{"displayName":"Rohan Deb","userId":"07798907649003110334"}},"outputId":"03effe3f-ab87-409e-ed67-d13c29734c37"},"source":["# load dataset\n","raw_dataset = load_clean_sentences('english-german.pkl')\n"," \n","# reduce dataset size\n","n_sentences = 10000\n","dataset = raw_dataset[:n_sentences, :]\n","# random shuffle\n","shuffle(dataset)\n","# split into train/test\n","train, test = dataset[:9000], dataset[9000:]\n","# save\n","save_clean_data(dataset, 'english-german-both.pkl')\n","save_clean_data(train, 'english-german-train.pkl')\n","save_clean_data(test, 'english-german-test.pkl')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: english-german-both.pkl\n","Saved: english-german-train.pkl\n","Saved: english-german-test.pkl\n"]}]},{"cell_type":"code","metadata":{"id":"JPW-i_9XJw83"},"source":["from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from keras.layers import Embedding\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.callbacks import ModelCheckpoint\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igyVRvoUJ9x9"},"source":["# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbmKMnvyKcuF"},"source":["# fit a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7-fW0AsKe3Z"},"source":["# max sentence length\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jlLFK9UcKgwx"},"source":["# encode and pad sequences\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_GjqK4bKi7j"},"source":["# one hot encode target sequence\n","def encode_output(sequences, vocab_size):\n","\tylist = list()\n","\tfor sequence in sequences:\n","\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n","\t\tylist.append(encoded)\n","\ty = array(ylist)\n","\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n","\treturn y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iqy4ncCdKlNm"},"source":["# define NMT model\n","def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n","\tmodel = Sequential()\n","\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps))\n","\tmodel.add(LSTM(n_units))\n","\tmodel.add(RepeatVector(tar_timesteps))\n","\tmodel.add(LSTM(n_units, return_sequences=True))\n","\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n","\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hiL6S3r1Knrq"},"source":["# load datasets\n","dataset = load_clean_sentences('english-german-both.pkl')\n","train = load_clean_sentences('english-german-train.pkl')\n","test = load_clean_sentences('english-german-test.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnwkjO3MKqd3","executionInfo":{"status":"ok","timestamp":1668305639636,"user_tz":360,"elapsed":315,"user":{"displayName":"Rohan Deb","userId":"07798907649003110334"}},"outputId":"956f7770-52ee-48fa-b537-e088bd5957f1"},"source":["# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","print('English Vocabulary Size: %d' % eng_vocab_size)\n","print('English Max Length: %d' % (eng_length))\n","\n","# prepare german tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","print('German Vocabulary Size: %d' % ger_vocab_size)\n","print('German Max Length: %d' % (ger_length))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["English Vocabulary Size: 2404\n","English Max Length: 5\n","German Vocabulary Size: 3856\n","German Max Length: 10\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSGbgzbaKsfV","executionInfo":{"status":"ok","timestamp":1668305641074,"user_tz":360,"elapsed":913,"user":{"displayName":"Rohan Deb","userId":"07798907649003110334"}},"outputId":"26404bef-da54-4de8-8dab-90cad2a86df4"},"source":["# prepare training data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n","print(trainY[0])\n","\n","trainY = encode_output(trainY, eng_vocab_size)\n","\n","print(trainY[0])\n","# prepare validation data\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n","testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n","testY = encode_output(testY, eng_vocab_size)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[  1 564   3   0   0]\n","[[0. 1. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4X4LQ7lvKwjS","executionInfo":{"status":"ok","timestamp":1668305643151,"user_tz":360,"elapsed":2080,"user":{"displayName":"Rohan Deb","userId":"07798907649003110334"}},"outputId":"3e9d0bd3-831c-4b36-d738-ef5fd1794cf9"},"source":["# define model\n","model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n","model.compile(optimizer='adam', loss='categorical_crossentropy')\n","# summarize defined model\n","print(model.summary())\n","#plot_model(model, to_file='model.png', show_shapes=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 10, 256)           987136    \n","                                                                 \n"," lstm (LSTM)                 (None, 256)               525312    \n","                                                                 \n"," repeat_vector (RepeatVector  (None, 5, 256)           0         \n"," )                                                               \n","                                                                 \n"," lstm_1 (LSTM)               (None, 5, 256)            525312    \n","                                                                 \n"," time_distributed (TimeDistr  (None, 5, 2404)          617828    \n"," ibuted)                                                         \n","                                                                 \n","=================================================================\n","Total params: 2,655,588\n","Trainable params: 2,655,588\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmlJBQJqKzeM","executionInfo":{"status":"ok","timestamp":1635583275150,"user_tz":-330,"elapsed":390350,"user":{"displayName":"Rohan Deb","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQnnmq2A_WzQddHXcH17ANlD5xMZadOyaQva1Ciw=s64","userId":"07798907649003110334"}},"outputId":"293ab606-bc8a-4379-b509-bda38062f0eb"},"source":["# fit model\n","filename = 'model.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(trainX, trainY, epochs=15, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","141/141 - 26s - loss: 0.4885 - val_loss: 2.2084\n","\n","Epoch 00001: val_loss improved from inf to 2.20839, saving model to model.h5\n","Epoch 2/15\n","141/141 - 26s - loss: 0.4460 - val_loss: 2.1981\n","\n","Epoch 00002: val_loss improved from 2.20839 to 2.19811, saving model to model.h5\n","Epoch 3/15\n","141/141 - 26s - loss: 0.4116 - val_loss: 2.2072\n","\n","Epoch 00003: val_loss did not improve from 2.19811\n","Epoch 4/15\n","141/141 - 26s - loss: 0.3785 - val_loss: 2.2070\n","\n","Epoch 00004: val_loss did not improve from 2.19811\n","Epoch 5/15\n","141/141 - 26s - loss: 0.3492 - val_loss: 2.2077\n","\n","Epoch 00005: val_loss did not improve from 2.19811\n","Epoch 6/15\n","141/141 - 25s - loss: 0.3241 - val_loss: 2.2292\n","\n","Epoch 00006: val_loss did not improve from 2.19811\n","Epoch 7/15\n","141/141 - 26s - loss: 0.2981 - val_loss: 2.2220\n","\n","Epoch 00007: val_loss did not improve from 2.19811\n","Epoch 8/15\n","141/141 - 26s - loss: 0.2763 - val_loss: 2.2347\n","\n","Epoch 00008: val_loss did not improve from 2.19811\n","Epoch 9/15\n","141/141 - 26s - loss: 0.2576 - val_loss: 2.2318\n","\n","Epoch 00009: val_loss did not improve from 2.19811\n","Epoch 10/15\n","141/141 - 26s - loss: 0.2402 - val_loss: 2.2507\n","\n","Epoch 00010: val_loss did not improve from 2.19811\n","Epoch 11/15\n","141/141 - 26s - loss: 0.2231 - val_loss: 2.2579\n","\n","Epoch 00011: val_loss did not improve from 2.19811\n","Epoch 12/15\n","141/141 - 26s - loss: 0.2091 - val_loss: 2.2685\n","\n","Epoch 00012: val_loss did not improve from 2.19811\n","Epoch 13/15\n","141/141 - 26s - loss: 0.1969 - val_loss: 2.2771\n","\n","Epoch 00013: val_loss did not improve from 2.19811\n","Epoch 14/15\n","141/141 - 26s - loss: 0.1840 - val_loss: 2.3028\n","\n","Epoch 00014: val_loss did not improve from 2.19811\n","Epoch 15/15\n","141/141 - 26s - loss: 0.1753 - val_loss: 2.2923\n","\n","Epoch 00015: val_loss did not improve from 2.19811\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f7614601390>"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXUGv5SZK-Qp","executionInfo":{"status":"ok","timestamp":1635586286734,"user_tz":-330,"elapsed":503614,"user":{"displayName":"Rohan Deb","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQnnmq2A_WzQddHXcH17ANlD5xMZadOyaQva1Ciw=s64","userId":"07798907649003110334"}},"outputId":"1750cbd4-ac78-4932-d5e2-38e774cf2f6e"},"source":["from pickle import load\n","from numpy import array\n","from numpy import argmax\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","# fit a tokenizer\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","# max sentence length\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)\n","\n","# encode and pad sequences\n","def encode_sequences(tokenizer, length, lines):\n","\t# integer encode sequences\n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n","\n","# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n","\n","# generate target given source sequence\n","def predict_sequence(model, tokenizer, source):\n","\tprediction = model.predict(source, verbose=0)[0]\n","\tintegers = [argmax(vector) for vector in prediction]\n","\ttarget = list()\n","\tfor i in integers:\n","\t\tword = word_for_id(i, tokenizer)\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\ttarget.append(word)\n","\treturn ' '.join(target)\n","\n","# evaluate the skill of the model\n","def evaluate_model(model, tokenizer, sources, raw_dataset):\n","\tactual, predicted = list(), list()\n","\tfor i, source in enumerate(sources):\n","\t\t# translate encoded source text\n","\t\tsource = source.reshape((1, source.shape[0]))\n","\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n","\t\traw_target, raw_src = raw_dataset[i]\n","\t\tif i < 10:\n","\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n","\t\tactual.append([raw_target.split()])\n","\t\tpredicted.append(translation.split())\n","\n","# load datasets\n","dataset = load_clean_sentences('english-german-both.pkl')\n","train = load_clean_sentences('english-german-train.pkl')\n","test = load_clean_sentences('english-german-test.pkl')\n","# prepare english tokenizer\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","# prepare german tokenizer\n","ger_tokenizer = create_tokenizer(dataset[:, 1])\n","ger_vocab_size = len(ger_tokenizer.word_index) + 1\n","ger_length = max_length(dataset[:, 1])\n","# prepare data\n","trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n","testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n","\n","# load model\n","model = load_model('model.h5')\n","# test on some training sequences\n","print('train')\n","evaluate_model(model, eng_tokenizer, trainX, train)\n","# test on some test sequences\n","print('test')\n","evaluate_model(model, eng_tokenizer, testX, test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train\n","src=[mir ging der sprit aus], target=[i ran out of gas], predicted=[i made up at]\n","src=[ich mochte viel], target=[i want a lot], predicted=[i want a lot]\n","src=[pragt es euch ein], target=[memorize it], predicted=[memorize it]\n","src=[gib mir ruckendeckung], target=[watch my back], predicted=[watch my back]\n","src=[was fur ein zirkus], target=[what a hassle], predicted=[what a hassle]\n","src=[ich will kinder], target=[i want kids], predicted=[i want kids]\n","src=[das ist meine arbeit], target=[this is my job], predicted=[its is job]\n","src=[erklaren sie mir das], target=[explain it to me], predicted=[explain it to me]\n","src=[tom ist fair], target=[tom is fair], predicted=[tom is fair]\n","src=[lassen sie mich los], target=[let go of me], predicted=[let go to me]\n","test\n","src=[ich habe die hoffnung verloren], target=[ive lost hope], predicted=[i lost my watch]\n","src=[ich habe einen tisch], target=[i have a table], predicted=[i have a a]\n","src=[hier bin ich], target=[here i am], predicted=[where is paris]\n","src=[wird sie kommen], target=[will she come], predicted=[keep them up]\n","src=[bitte sag es mir], target=[please tell me], predicted=[please tell me]\n","src=[konnen sie mich sehen], target=[can they see me], predicted=[can you see that]\n","src=[tom ist schallplattenunterhalter], target=[tom is a dj], predicted=[tom is mad]\n","src=[was ist liebe], target=[what is love], predicted=[how is]\n","src=[der brunnen ist trocken], target=[the well is dry], predicted=[the the engine]\n","src=[sei nicht solch ein faulpelz], target=[dont be so lazy], predicted=[dont be died]\n"]}]},{"cell_type":"code","metadata":{"id":"S_UXNdirY1hx"},"source":[],"execution_count":null,"outputs":[]}]}